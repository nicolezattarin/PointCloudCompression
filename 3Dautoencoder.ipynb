{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5t6WuwPwHN4k"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import open3d as o3d\n",
        "from pyntcloud import PyntCloud\n",
        "import glob\n",
        "import plyfile as ply\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.utils import save_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "my70l0TOHN4n"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ThreeD_conv(nn.Module):\n",
        "    def enc_linear (self,flatten_size):\n",
        "        # print ('flatten_size', flatten_size)\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(flatten_size, 32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(32, self.code_size),\n",
        "        )\n",
        "\n",
        "    def dec_linear (self, flatten_size):\n",
        "        return nn.Sequential(\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(self.code_size, 32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(32, flatten_size),\n",
        "        )\n",
        "\n",
        "    def unflatten (self, unflattened_size):\n",
        "        return nn.Unflatten(dim=1, unflattened_size=unflattened_size)\n",
        "\n",
        "    def __init__(self, code_size=100):\n",
        "        super(ThreeD_conv, self).__init__()\n",
        "\n",
        "        self.code_size = code_size\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv3d(in_channels=1, out_channels=16, kernel_size=4, padding=1, bias=False),\n",
        "            nn.BatchNorm3d(16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(in_channels=16, out_channels=32, kernel_size=4, padding=1, bias=False),\n",
        "            nn.BatchNorm3d(32),            \n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(in_channels=32, out_channels=64, kernel_size=4, padding=1, bias=False),\n",
        "            nn.BatchNorm3d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.flatten = nn.Flatten(start_dim=1)\n",
        "        \n",
        "        \n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose3d(in_channels=64, out_channels=32, kernel_size=4, padding=1, bias=False),\n",
        "            nn.BatchNorm3d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose3d(in_channels=32, out_channels=16, kernel_size=4, padding=1, bias=False),\n",
        "            nn.BatchNorm3d(16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose3d(in_channels=16, out_channels=1, kernel_size=4, padding=1, bias=False),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.encoder(input)\n",
        "        unflattened_size = x.shape[1:]\n",
        "        x = self.flatten(x)\n",
        "        flattened_size = x.shape[1]\n",
        "        x = self.enc_linear(flattened_size)(x)\n",
        "        encoded_data = x\n",
        "        x = self.dec_linear(flattened_size)(x)\n",
        "        x = self.unflatten(unflattened_size)(x)\n",
        "        x = self.decoder(x)\n",
        "        return encoded_data, x\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "T5cndGV_HN4p"
      },
      "outputs": [],
      "source": [
        "import plyfile \n",
        "class LidarDataset (Dataset):\n",
        "    def __init__(self, all_files, transform=None):\n",
        "        \"\"\"2DRepresentation\n",
        "\n",
        "        Args:\n",
        "            folder_pattern (string): Path to the folder with all the images. regex pattern\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "    \n",
        "        self.transform = transform\n",
        "        # Read the file and get all the lines\n",
        "        self.data = [] # list of images\n",
        "\n",
        "        for file in all_files:\n",
        "            img = plyfile.PlyData.read(file)            \n",
        "            img = np.column_stack((img['vertex']['x'],img['vertex']['y'],img['vertex']['z']))\n",
        "            \n",
        "            # remove last column, which is the label\n",
        "            img = img[:,0:3]\n",
        "            self.data.append(img)\n",
        "            \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.data[idx]\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QpTDAj5HN4q",
        "outputId": "ea9a2adc-6955-47bc-df5c-116a380ada85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of files:  50\n"
          ]
        }
      ],
      "source": [
        "import open3d as o3d\n",
        "def transform(point_cloud, image_size=(128, 128, 80)):\n",
        "\n",
        "    voxel_size = 2\n",
        "    #resha\n",
        "    pcd = o3d.geometry.PointCloud()\n",
        "    pcd.points = o3d.utility.Vector3dVector(point_cloud)\n",
        "    voxels = o3d.geometry.VoxelGrid.create_from_point_cloud(pcd, voxel_size=voxel_size)\n",
        "    \n",
        "    # get torch voxels\n",
        "    voxels = voxels.get_voxels() \n",
        "\n",
        "    v = np.array([])\n",
        "    for voxel in voxels:\n",
        "        v = np.append(v, voxel.grid_index)\n",
        "    v = v.reshape(-1, 3)\n",
        "\n",
        "    data = np.zeros(image_size, dtype=np.int32)\n",
        "    # center the voxel in order to have the center in the middle of the image\n",
        "    minx = np.min(v[:,0])\n",
        "    miny = np.min(v[:,1])\n",
        "    minz = np.min(v[:,2])\n",
        "\n",
        "    v[:,0] = v[:,0] - minx\n",
        "    v[:,1] = v[:,1] - miny\n",
        "    v[:,2] = v[:,2] - minz\n",
        "\n",
        "    for voxel in v:\n",
        "        try:\n",
        "            data[int(voxel[0]), int(voxel[1]), int(voxel[2])] = 1\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    data = torch.from_numpy(data).double()\n",
        "    data = data.unsqueeze(0)\n",
        "\n",
        "    return data\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    file_path = \"/content/drive/My Drive/3DAR/dataset_final/dataset_final/*.ply\"\n",
        "except:\n",
        "    file_path = \"dataset-downloader-kit/CV/dataset/Town01_Opt_ClearSunset/dataset_final/*.ply\"\n",
        "\n",
        "\n",
        "train_test_split = 0.8\n",
        "\n",
        "all_files = glob.glob(file_path)[:50]\n",
        "all_files.sort()\n",
        "print (\"Total number of files: \", len(all_files))\n",
        "\n",
        "train_files = all_files[:int(len(all_files)*train_test_split)]\n",
        "test_files = all_files[int(len(all_files)*train_test_split):]\n",
        "\n",
        "batch_size = 6\n",
        "train_dataset = LidarDataset(train_files, transform=transform)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
        "\n",
        "test_dataset = LidarDataset(test_files, transform=transform)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "kWw51Z__HN4r"
      },
      "outputs": [],
      "source": [
        "### Training function\n",
        "def train_epoch(autoencoder, device, dataloader, loss_fn, optimizer):\n",
        "    # Set train mode for both the encoder and the decoder\n",
        "    autoencoder.train()\n",
        "    autoencoder = autoencoder.float()\n",
        "    losses = []\n",
        "\n",
        "    # Iterate the dataloader (we do not need the label values, this is unsupervised learning)\n",
        "    for image_batch in dataloader: \n",
        "        image_batch = image_batch.to(device)\n",
        "        print ('image_batch.shape', image_batch.shape)\n",
        "        encoded_data, decoded_data = autoencoder(image_batch.float())\n",
        "        # convert to float\n",
        "        decoded_data = decoded_data.float()\n",
        "        image_batch = image_batch.float()\n",
        "\n",
        "        loss = loss_fn(decoded_data, image_batch)\n",
        "        \n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        losses.append(loss.detach().cpu().numpy())\n",
        "    losses = np.mean(losses)\n",
        "    return losses\n",
        "\n",
        "### Testing function\n",
        "def test_epoch(autoencoder, device, dataloader, loss_fn):\n",
        "    # Set evaluation mode for encoder and decoder\n",
        "    autoencoder.train()\n",
        "    autoencoder = autoencoder.float()\n",
        "    with torch.no_grad(): # No need to track the gradients\n",
        "        # Define the lists to store the outputs for each batch\n",
        "        conc_out = []\n",
        "        conc_label = []\n",
        "        for image_batch in dataloader:\n",
        "            # Move tensor to the proper device\n",
        "            image_batch = image_batch.to(device)\n",
        "            # Encode data\n",
        "            encoded_data, decoded_data = autoencoder(image_batch.float())\n",
        "            \n",
        "            # Append the network output and the original image to the lists\n",
        "            conc_out.append(decoded_data.cpu())\n",
        "            conc_label.append(image_batch.cpu())\n",
        "        # Create a single tensor with all the values in the lists\n",
        "        conc_out = torch.cat(conc_out)\n",
        "        conc_label = torch.cat(conc_label) \n",
        "        # Evaluate global loss\n",
        "        val_loss = loss_fn(conc_out, conc_label)\n",
        "    return val_loss.data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWrz-G38HN4s",
        "outputId": "e942d306-4e53-4857-a166-b7ce644f18fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ThreeD_conv(\n",
              "  (encoder): Sequential(\n",
              "    (0): Conv3d(1, 16, kernel_size=(4, 4, 4), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "    (1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Conv3d(16, 32, kernel_size=(4, 4, 4), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "    (4): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Conv3d(32, 64, kernel_size=(4, 4, 4), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "    (7): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (8): ReLU(inplace=True)\n",
              "  )\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (decoder): Sequential(\n",
              "    (0): ConvTranspose3d(64, 32, kernel_size=(4, 4, 4), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "    (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): ConvTranspose3d(32, 16, kernel_size=(4, 4, 4), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "    (4): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): ConvTranspose3d(16, 1, kernel_size=(4, 4, 4), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
              "    (7): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "### Define the loss function\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "AE = ThreeD_conv(100)\n",
        "\n",
        "### Define an optimizer (both for the encoder and the decoder!)\n",
        "lr = 0.005\n",
        "params_to_optimize = [\n",
        "    {'params': AE.parameters(), 'lr': lr}\n",
        "]\n",
        "optim = torch.optim.Adam(params_to_optimize, lr=lr)\n",
        "\n",
        "# Check if the GPU is available\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "# print(f'Selected device: {device}')\n",
        "\n",
        "# Move both the encoder and the decoder to the selected device\n",
        "AE.to(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHRUEHh5HN4s",
        "outputId": "81d060ec-0a14-48d1-8818-e30d2c4da48d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 1/30\n",
            "image_batch.shape torch.Size([6, 1, 128, 128, 80])\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mCanceled future for execute_request message before replies were done"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "### Training cycle\n",
        "num_epochs = 30\n",
        "for epoch in range(num_epochs):\n",
        "    print('EPOCH %d/%d' % (epoch + 1, num_epochs))\n",
        "    ### Training (use the training function)\n",
        "    train_loss = train_epoch(\n",
        "        autoencoder=AE,\n",
        "        device=device, \n",
        "        dataloader=train_dataloader, \n",
        "        loss_fn=loss_fn, \n",
        "        optimizer=optim)\n",
        "    print(f'TRAIN - EPOCH {epoch+1}/{num_epochs} - loss: {train_loss}')\n",
        "\n",
        "    ### Validation  (use the testing function)\n",
        "    val_loss = test_epoch(\n",
        "        autoencoder=AE,\n",
        "        device=device, \n",
        "        dataloader=test_dataloader, \n",
        "        loss_fn=loss_fn)\n",
        "    # Print Validationloss\n",
        "    print(f'VALIDATION - EPOCH {epoch+1}/{num_epochs} - loss: {val_loss}\\n')\n",
        "\n",
        "\n",
        "    # Save network parameters\n",
        "    torch.save(AE.state_dict(), 'AE_params.pth')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "3d",
      "language": "python",
      "name": "3d"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "e6475e3dda00584803999dc22275b04436509c38667d0d0b646610abad6aff18"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
