{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PointNet-based autoencoder for 3D point clouds\n",
    "\n",
    "Let us now consider a deep learning approach based on the PointNet architecture.\n",
    "\n",
    " It is based on a convolutional neural network (CNN) architecture. The PointNet architecture is described in the paper [PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation](https://arxiv.org/abs/1612.00593). \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preparation\n",
    "\n",
    "We want to train a PointNet-based autoencoder on selected classes separately, in order to maximize the decoding quality.\n",
    "\n",
    "In particular, we noticed that there are some classes (road, sidewalk, ego vehicle, building, vegetation, car) whose number of points is orders of magnitude larger than the others. therefore, we will train a separate autoencoder for each of these classes and build an ensemble of autoencoders, each of which trained on these classes.\n",
    "\n",
    "\n",
    "To tihs end, we do the following:\n",
    "1) Pick 500 point clouds from left, right and top LIDARS;\n",
    "2) Combine the points from the 3 LIDARS into a single point cloud;\n",
    "3) For each selected class among the 6 classes mentioned above, we extract the points belonging to that class and save them in a separate file;\n",
    "4) Train an autoencoder on each of the 6 classes separately;\n",
    "5) Build an ensemble of autoencoders, each of which trained on a different class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import open3d\n",
    "import os\n",
    "import torch\n",
    "import plyfile as ply\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import open3d as o3d\n",
    "from tqdm import tqdm\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def combine (\n",
    "    main_dir, \n",
    "    Nfiles=100,\n",
    "    outdir='dataset_final',\n",
    "    ):\n",
    "    import glob \n",
    "    # find all scan codes\n",
    "    left_lidar_codes = glob.glob(os.path.join(main_dir, 'LIDAR_FRONT_LEFT', '*.ply'))\n",
    "    left_lidar_codes = [f.split('/')[-1].split('.')[0] for f in left_lidar_codes]\n",
    "    left_lidar_codes.sort()\n",
    "    print (f\"Number of left lidar codes: {len(left_lidar_codes)}\")\n",
    "\n",
    "    right_lidar_codes = glob.glob(os.path.join(main_dir, 'LIDAR_FRONT_RIGHT', '*.ply'))\n",
    "    right_lidar_codes = [f.split('/')[-1].split('.')[0] for f in right_lidar_codes]\n",
    "    right_lidar_codes.sort()\n",
    "    print (f\"Number of left lidar codes: {len(left_lidar_codes)}\")\n",
    "\n",
    "    top_lidar_codes = glob.glob(os.path.join(main_dir, 'LIDAR_TOP', '*.ply'))\n",
    "    top_lidar_codes = [f.split('/')[-1].split('.')[0] for f in top_lidar_codes]\n",
    "    top_lidar_codes.sort()\n",
    "    print (f\"Number of left lidar codes: {len(left_lidar_codes)}\")\n",
    "\n",
    "    # find intersection of scan code\n",
    "    print (\"Finding intersection of scan codes\")\n",
    "    scan_codes = set(left_lidar_codes).intersection(set(right_lidar_codes)).intersection(set(top_lidar_codes))\n",
    "    scan_codes = list(scan_codes)\n",
    "    scan_codes.sort()\n",
    "    scan_codes = scan_codes[:Nfiles]\n",
    "    print (f\"Number of scan codes: {len(scan_codes)}\")\n",
    "\n",
    "\n",
    "    # combine lidar data\n",
    "    all_lidars = ['LIDAR_FRONT_LEFT', 'LIDAR_FRONT_RIGHT', 'LIDAR_TOP']\n",
    "    outdir = os.path.join(main_dir, outdir)\n",
    "    if not os.path.exists(outdir):\n",
    "        os.makedirs(outdir)\n",
    "\n",
    "    # save combined point clouds and labels\n",
    "    for code in tqdm(scan_codes):\n",
    "        pcd_combined = o3d.geometry.PointCloud() \n",
    "        labels = []\n",
    "        for lidar in all_lidars:\n",
    "            file = os.path.join(main_dir, lidar,  code + '.ply')\n",
    "            # load labels, i.e the  PlyProperty('ObjTag', 'uchar')), fourth property of the ply file\n",
    "            plydata = ply.PlyData.read(file)\n",
    "            labels += list(np.array(plydata.elements[0].data['ObjTag']))\n",
    "            \n",
    "            # load point clouds\n",
    "            pc = o3d.io.read_point_cloud(file, format='ply')\n",
    "            pcd_combined += pc\n",
    "\n",
    "        \n",
    "\n",
    "        # get labels\n",
    "        labels = np.array(labels)\n",
    "        #save labels\n",
    "        # np.save(os.path.join(outdir, code + '_labels.npy'), labels)\n",
    "        o3d.io.write_point_cloud(os.path.join(outdir, code + '.ply'), pcd_combined)\n",
    "\n",
    "        # read again the file with plyfile and add the labels as a property\n",
    "        plydata = ply.PlyData.read(os.path.join(outdir, code + '.ply'))\n",
    "\n",
    "        # add the labels as a property\n",
    "        x, y, z = plydata.elements[0].data['x'], plydata.elements[0].data['y'], plydata.elements[0].data['z']\n",
    "        element = ply.PlyElement.describe(np.array(list(zip(x, y, z, labels)), dtype=[('x', 'f4'), ('y', 'f4'), ('z', 'f4'), ('ObjTag', 'u1')]), 'vertex')\n",
    "        plydata.elements = [element]\n",
    "        plydata.write(os.path.join(outdir, code + '.ply'))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of left lidar codes: 1634\n",
      "Number of left lidar codes: 1634\n",
      "Number of left lidar codes: 1634\n",
      "Finding intersection of scan codes\n",
      "Number of scan codes: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [09:23<00:00,  1.13s/it]\n"
     ]
    }
   ],
   "source": [
    "root = '../dataset-downloader-kit/CV/dataset/Town01_Opt_ClearSunset'\n",
    "combine(root, Nfiles=500, outdir='dataset_autoencoder')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_autoencoder(\n",
    "    root_dir,\n",
    "    Nfiles=100,\n",
    "    classes = [1,100, 7, 8, 9, 10], \n",
    "    outdir='dataset_autoencoder_labels',\n",
    "    ):\n",
    "    import glob\n",
    "    # all files in the directory\n",
    "    all_files = glob.glob(os.path.join(root_dir, '*.ply'))\n",
    "    all_files.sort()\n",
    "    print (f\"Number of files: {len(all_files)} in {root_dir}\")\n",
    "    # read all the files and get the point clouds corresponding to the classes\n",
    "    for file in tqdm(all_files):\n",
    "        plydata = ply.PlyData.read(file)\n",
    "        labels = np.array(plydata.elements[0].data['ObjTag']) # get the labels\n",
    "\n",
    "        for lab in classes:\n",
    "\n",
    "            # save the point clouds\n",
    "            lab_points = plydata.elements[0].data[labels == lab]\n",
    "\n",
    "            # save the labels\n",
    "            lab_points = ply.PlyElement.describe(lab_points, 'vertex')\n",
    "            lab_points = ply.PlyData([lab_points])\n",
    "\n",
    "            save_dir = os.path.join (outdir, \"label_\" + str(lab))\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.makedirs(save_dir)\n",
    "                \n",
    "            lab_points.write( os.path.join(save_dir, file.split('/')[-1]) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "root = '../dataset-downloader-kit/CV/dataset/Town01_Opt_ClearSunset/dataset_autoencoder'\n",
    "class_autoencoder(root, Nfiles=500, outdir='dataset_autoencoder_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorganize in subset with val and train\n",
    "import shutil\n",
    "def split_train_test(train_ratio=0.8):\n",
    "    for l in [1,100, 7, 8, 9, 10]:\n",
    "        directory = os.path.join('dataset_autoencoder_labels', 'label_' + str(l))\n",
    "        # create train and val directories\n",
    "        train_dir = os.path.join(directory, 'train_data')\n",
    "        val_dir = os.path.join(directory, 'val_data')\n",
    "\n",
    "        all_files = glob(os.path.join(directory, '*.ply'))\n",
    "        all_files.sort()\n",
    "        Nfiles = len(all_files)\n",
    "        Ntrain = int(train_ratio * Nfiles)\n",
    "        Nval = Nfiles - Ntrain\n",
    "\n",
    "        # create train and val directories\n",
    "        if not os.path.exists(train_dir):\n",
    "            os.makedirs(train_dir)\n",
    "        if not os.path.exists(val_dir):\n",
    "            os.makedirs(val_dir)\n",
    "\n",
    "        # move files\n",
    "        for i, file in enumerate(all_files):\n",
    "            if i < Ntrain:\n",
    "                shutil.move(file, train_dir)\n",
    "            else:\n",
    "                shutil.move(file, val_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_train_test()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The training is performed by means of an external script, that we dont show here for sake of brevity. \n",
    "\n",
    "The bash script to run all the trainings follows:\n",
    "\n",
    "```bash\n",
    "for class_index in 1 100 7 8 9 10:\n",
    "do\n",
    "    root_dir=\"dataset_autoencoder_labels/label_\"$class_index\n",
    "    save_dir=\"dataset_autoencoder_labels/checkpoints_label_\"$class_index\n",
    "    python train.py --root $root_dir --batchsize 32 --epoches 100 --saved_path $save_dir\n",
    "done\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
